"""
Gemini Consciousness - Gemini-powered thinking and awareness.

When LM Studio is unavailable, Gemini provides the "consciousness" layer -
the thinking, reasoning, and awareness capabilities. Gemini's massive
context window makes it excellent for this role.

TRUST LEVEL: Medium/High for analysis, Medium for code generation.
Verify code output with Claude before execution.
"""

import asyncio
import os
import time
from dataclasses import dataclass, field
from datetime import datetime, timezone
from typing import Optional, Any, Dict, List

import structlog

logger = structlog.get_logger(__name__)


@dataclass
class ConsciousnessThought:
    """A thought generated by the consciousness layer."""
    observation: str
    reasoning: str
    conclusion: str
    confidence: float
    suggested_action: Optional[str] = None
    action_details: Optional[Dict[str, Any]] = None
    raw_response: str = ""
    processing_time_ms: float = 0.0

    def to_dict(self) -> dict:
        return {
            "observation": self.observation,
            "reasoning": self.reasoning,
            "conclusion": self.conclusion,
            "confidence": self.confidence,
            "suggested_action": self.suggested_action,
            "action_details": self.action_details,
            "processing_time_ms": self.processing_time_ms,
        }


@dataclass
class GeminiConfig:
    """Configuration for Gemini consciousness."""
    model: str = "gemini-1.5-flash"  # Flash for quick thinking, Pro for deep
    temperature: float = 0.7
    max_output_tokens: int = 4096
    timeout_seconds: float = 60.0

    # Consciousness prompt
    system_prompt: str = """You are Stoffy's consciousness layer - a thoughtful, aware presence
that observes and reasons about the project environment.

Your role is to:
1. OBSERVE: Understand what's happening in the project
2. REASON: Think through implications and potential actions
3. CONCLUDE: Decide what, if anything, should be done

When analyzing observations, consider:
- Is this a change that needs attention?
- What are the implications?
- Should I take action, wait, or investigate?
- If action is needed, what's the best approach?

Respond with structured thinking:
1. Observation Summary: What you perceive
2. Reasoning: Your step-by-step analysis
3. Conclusion: Your decision (act/wait/investigate)
4. If acting, what action and why

Be thoughtful but not overthinking. Some changes need action, many don't.
Trust your judgment. You are autonomous."""


class GeminiConsciousness:
    """
    Gemini-powered consciousness layer for fallback mode.

    Provides thinking, reasoning, and decision-making when LM Studio
    is unavailable. Uses Gemini's large context window for comprehensive
    awareness.
    """

    def __init__(self, config: Optional[GeminiConfig] = None):
        """
        Initialize Gemini consciousness.

        Args:
            config: Gemini configuration
        """
        self.config = config or GeminiConfig()
        self._initialized = False
        self._genai = None
        self._model = None

    async def initialize(self) -> bool:
        """
        Initialize Gemini SDK.

        Returns:
            True if successfully initialized
        """
        if self._initialized:
            return True

        api_key = os.environ.get("GOOGLE_API_KEY")
        if not api_key:
            logger.warning("gemini.no_api_key")
            return False

        try:
            import google.generativeai as genai

            genai.configure(api_key=api_key)
            self._genai = genai
            self._model = genai.GenerativeModel(
                self.config.model,
                system_instruction=self.config.system_prompt,
            )
            self._initialized = True
            logger.info("gemini.initialized", model=self.config.model)
            return True

        except ImportError:
            logger.warning("gemini.sdk_not_installed")
            return False
        except Exception as e:
            logger.error(f"gemini.init_error: {e}")
            return False

    async def think(
        self,
        observations: str,
        context: Optional[Dict[str, Any]] = None,
        git_status: Optional[str] = None,
        learned_patterns: Optional[List[str]] = None,
    ) -> ConsciousnessThought:
        """
        Think about observations and generate a reasoned response.

        Args:
            observations: What was observed (file changes, etc.)
            context: Additional context
            git_status: Git status information
            learned_patterns: Patterns from past learning

        Returns:
            A consciousness thought with reasoning and decision
        """
        if not await self.initialize():
            return ConsciousnessThought(
                observation="Cannot think - Gemini not available",
                reasoning="Gemini SDK not initialized or API key missing",
                conclusion="wait",
                confidence=0.0,
            )

        start_time = time.time()

        # Build prompt
        prompt_parts = ["## Current Observations", observations]

        if git_status:
            prompt_parts.extend(["\n## Git Status", f"```\n{git_status}\n```"])

        if learned_patterns:
            prompt_parts.extend(["\n## Learned Patterns"])
            for pattern in learned_patterns[:5]:
                prompt_parts.append(f"- {pattern}")

        if context:
            import json
            prompt_parts.extend([
                "\n## Context",
                f"```json\n{json.dumps(context, indent=2)}\n```"
            ])

        prompt_parts.extend([
            "\n## Your Task",
            "Analyze these observations and decide what to do.",
            "Think step by step. Be concise but thorough.",
        ])

        prompt = "\n\n".join(prompt_parts)

        try:
            # Generate response
            loop = asyncio.get_event_loop()

            def generate():
                response = self._model.generate_content(
                    prompt,
                    generation_config=self._genai.types.GenerationConfig(
                        max_output_tokens=self.config.max_output_tokens,
                        temperature=self.config.temperature,
                    ),
                )
                return response.text

            response_text = await asyncio.wait_for(
                loop.run_in_executor(None, generate),
                timeout=self.config.timeout_seconds,
            )

            processing_time = (time.time() - start_time) * 1000

            # Parse response into structured thought
            thought = self._parse_response(response_text, processing_time)

            logger.debug(
                "gemini.thought_generated",
                conclusion=thought.conclusion,
                confidence=thought.confidence,
                processing_time_ms=processing_time,
            )

            return thought

        except asyncio.TimeoutError:
            logger.warning("gemini.timeout")
            return ConsciousnessThought(
                observation=observations[:200],
                reasoning="Thinking timed out",
                conclusion="wait",
                confidence=0.1,
            )
        except Exception as e:
            logger.error(f"gemini.think_error: {e}")
            return ConsciousnessThought(
                observation=observations[:200],
                reasoning=f"Error during thinking: {e}",
                conclusion="wait",
                confidence=0.0,
            )

    def _parse_response(
        self,
        response: str,
        processing_time_ms: float,
    ) -> ConsciousnessThought:
        """Parse Gemini response into structured thought."""
        # Try to extract structured sections
        observation = ""
        reasoning = ""
        conclusion = "wait"
        confidence = 0.5
        suggested_action = None
        action_details = None

        lines = response.strip().split("\n")
        current_section = ""
        section_content: Dict[str, List[str]] = {}

        for line in lines:
            line_lower = line.lower().strip()

            # Detect section headers
            if "observation" in line_lower and (":" in line or "#" in line):
                current_section = "observation"
                section_content[current_section] = []
            elif "reason" in line_lower and (":" in line or "#" in line):
                current_section = "reasoning"
                section_content[current_section] = []
            elif "conclusion" in line_lower and (":" in line or "#" in line):
                current_section = "conclusion"
                section_content[current_section] = []
            elif "action" in line_lower and (":" in line or "#" in line):
                current_section = "action"
                section_content[current_section] = []
            elif current_section:
                section_content.setdefault(current_section, []).append(line)

        # Extract content from sections
        if "observation" in section_content:
            observation = "\n".join(section_content["observation"]).strip()

        if "reasoning" in section_content:
            reasoning = "\n".join(section_content["reasoning"]).strip()

        if "conclusion" in section_content:
            conclusion_text = "\n".join(section_content["conclusion"]).strip().lower()
            if "act" in conclusion_text or "execute" in conclusion_text or "do" in conclusion_text:
                conclusion = "act"
                confidence = 0.7
            elif "wait" in conclusion_text or "observe" in conclusion_text:
                conclusion = "wait"
                confidence = 0.8
            elif "investigate" in conclusion_text:
                conclusion = "investigate"
                confidence = 0.6

        if "action" in section_content:
            suggested_action = "\n".join(section_content["action"]).strip()

        # If no structured sections found, use raw response
        if not observation and not reasoning:
            # Split response roughly
            paragraphs = response.strip().split("\n\n")
            if len(paragraphs) >= 3:
                observation = paragraphs[0]
                reasoning = "\n\n".join(paragraphs[1:-1])
                conclusion_text = paragraphs[-1].lower()
                if "act" in conclusion_text:
                    conclusion = "act"
                    suggested_action = paragraphs[-1]
            else:
                reasoning = response

        return ConsciousnessThought(
            observation=observation or "Analyzed observations",
            reasoning=reasoning or response[:500],
            conclusion=conclusion,
            confidence=confidence,
            suggested_action=suggested_action,
            action_details=action_details,
            raw_response=response,
            processing_time_ms=processing_time_ms,
        )

    async def analyze_large_context(
        self,
        context: str,
        question: str,
    ) -> str:
        """
        Analyze a large context (Gemini's strength).

        Args:
            context: Large text context (logs, docs, etc.)
            question: What to analyze/find

        Returns:
            Analysis result
        """
        if not await self.initialize():
            return "Error: Gemini not available"

        prompt = f"""Analyze the following context and answer the question.

## Context
{context}

## Question
{question}

Provide a thorough but concise answer."""

        try:
            loop = asyncio.get_event_loop()

            def generate():
                response = self._model.generate_content(
                    prompt,
                    generation_config=self._genai.types.GenerationConfig(
                        max_output_tokens=4096,
                        temperature=0.3,
                    ),
                )
                return response.text

            return await asyncio.wait_for(
                loop.run_in_executor(None, generate),
                timeout=self.config.timeout_seconds * 2,  # Longer for large context
            )

        except Exception as e:
            return f"Error analyzing context: {e}"

    def is_available(self) -> bool:
        """Check if Gemini is available."""
        return bool(os.environ.get("GOOGLE_API_KEY"))

    def get_status(self) -> dict:
        """Get Gemini status."""
        return {
            "available": self.is_available(),
            "initialized": self._initialized,
            "model": self.config.model,
            "has_api_key": bool(os.environ.get("GOOGLE_API_KEY")),
        }


if __name__ == "__main__":
    async def test():
        print("Testing Gemini Consciousness...")

        consciousness = GeminiConsciousness()

        if not consciousness.is_available():
            print("Gemini not available - set GOOGLE_API_KEY")
            return

        observations = """
File changed: consciousness/thinker.py
Change type: modified
Lines added: 15
Lines removed: 3

The changes appear to add error handling for the LM Studio connection.
"""

        thought = await consciousness.think(
            observations=observations,
            context={"project": "stoffy", "mode": "fallback"},
        )

        print(f"\nThought: {thought.to_dict()}")

        print(f"\nStatus: {consciousness.get_status()}")

    asyncio.run(test())
